\chapter{Обзор литературы} \label{ch2}
	
% не рекомендуется использовать отдельную section <<введение>> после лета 2020 года
%\section{Введение} \label{ch2:intro}

Исследуемая модель является Content-Based моделью рекомендаций, так как полученные вектора помимо своих характеристик хранят в себе и информацию о семантическом отношении в сессии пользователя.
Поэтому в данном параграфе будут рассматриваться популярные модели получения векторов и методы рекомендации основанные на модели.

\section{Модели получения векторного представления слова}
\subsection{Word2Vec (SGNS)} 

В основе работы данного алгоритма лежит идея о моделирование условного распределения вероятностей соседних слов. Также стоит отметить, что в отличие от других моделей дистрибутивной семантики (GloVe), Word2Vec работает с последовательностью слов, находящиеся от центрального слова на заданном расстояние - ширина окна.\\
В рассматриваемой модели хранятся и настраиваются два вектора для каждого слова. Первый вектор - является центральным представлением слова в рассматриваемом окне. Второй вектор - является контекстным представлением слова. \\
Для поиска оптимума в пространстве параметров данной модели используется градиентный спуск. \\

Skip Gram - предсказываем соседние слова по центральному слову\cite{word2vec}:

\begin{equation}
	W, D \in \mathbb{R}^{Vocab \times EmbSize} \\
	\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \rightarrow \max_{W,D}
\end{equation}

Стоит отметить, что сумма в вышеописанном выражение идет не по всем уникальным словам, а по всем возможным словоупотреблениям.
    
Мы предполагаем, что соседние слова условно независимы друг от друга, когда мы уже рассмотрели центральное слово. 

\begin{equation}
	P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \prod_j P(CtxW_j | CenterW_i; W, D)
\end{equation}

Тогда наше распределение можно представить в виде произведения более простых распределений.

Каждое такое более простое распределение, будем моделировать при помощи softmax.


\begin{equation}
	P(CtxW_j | CenterW_i; W, D) = \frac{e^{w_i \cdot d_j}} { \sum_{j=1}^{|V|} e^{w_i \cdot d_j}} = softmax
\end{equation}


Из-за наличия в знаменателе суммы по всем объектам нашей выборки, каждый шаг градиентного спуска обходится вычислительно затратно. 
     
Поэтому будем использовать аппроксимацию negative sampling (отрицательное сэмплирование)

\begin{equation}
	P(CtxW_j | CenterW_i; W, D) \simeq \frac{e^{w_i \cdot d_j^+}} { \sum_{j=1}^{k} e^{w_i \cdot d_j^-}}, \quad k \ll |V|
\end{equation}


Суть данной аппроксимации заключается в том, что мы будем считать скалярное произведение в знаменателе не по всей нашей выборки объектов, а лишь по некоторым случайно выбранным.

\subsection{TF-IDF}

Данный подкласс моделей еще называется "мешком слов". Главная идея таких алгоритмов это то, что тематика текста хорошо описывается не порядком слов в документе, а
составом лексикона и частотой встречаемости слов.

Тогда каждый документ описывается разреженным вектором.
Для того, чтобы модель адекватно описывала данные необходимо, чтобы у каждого слова был свой вес. Одним из методов подсчета веса слова и является метод TF-IDF.

Основная идея в том, что чем чаще слово встречается в документе, тем более характерно оно для этого документа.
С другой стороны чем реже встречается слово в выборке документов, тем оно более специфично и информативно.

TF - term frequency - значимость слова в рамках документа \cite{tfidf}

\begin{equation}
	TF(w, d) = \frac{WordCount(w, d)}{Length(d)} 
\end{equation}

где $WordCount(w, d)$ - количество употреблений слова w в документе d,
$Length(d)$ - длина документа d в словах. 


IDF - inverse document frequency - специфичность слова \cite{tfidf}

\begin{equation}
	IDF(w, c) = \frac{Size(c)}{DocCount(w, c)}
\end{equation}

где $DocCount(w, c)$ - количество документов в коллекции c, в которых встречается слово w,

$Size(c)$ - размер коллекции c в документах.

Тогда вес слова подсчитывается следующим образом:


\begin{equation}
	TFIDF(w, d, c) = TF(w, d) * IDF(w, c)
\end{equation}

\section{Модель рекомендаций}
Главная цель моделей рекомендаций - это моделирование отношения между поведением пользователя и товарами или услугами предоставляемыми сервисами.

Отношение между пользователем и объектами можно приблизить некоторыми числами, которые описывают параметры пользователя и параметры объектов. Таким образом образуются векторы в пространстве одной и той же размерности,
при этом потребовав, чтобы скалярное произведение вектора, описывающего пользователя, и вектора, описывающий объект, хорошо приближала оценку отношений.
\begin{equation}
	x_{ij} \approx \langle u_i, \nu_j \rangle
\end{equation}
$u_i$ - параметры пользователя

$\nu_j$ - параметры объектов

Таким образом мы перешли к оптимизационной задаче:

\begin{equation}
	\sum (\langle u_i, \nu_j \rangle - x_{ij})^{2} \rightarrow min
\end{equation}

\subsection{SVD (Singular Value Decomposition)}

Пусть дана матрица пользователи - объекты, на пересечении которых стоят оценки пользователей.

Данная матрица имеет огромный размер (количество пользователей интернет ресурса может достигать нескольких миллионов, как и количество объектов предоставляемых веб-ресурсом)

Для любой вещественной $(n \times n)$ – матрицы $А$ существуют две вещественные ортогональные матрицы $U$ и $V$ такие, что 
\begin{equation}
	U^{T}AV = \varLambda 
\end{equation} \cite{svd}

Используя SVD-разложение матрицы пользователи - объекты, мы получим 2 матрицы: $U$ $n \times  k$  и $V$ $m \times k$, где
n - число пользователей, m -  число объектов, k - набор факторов.
Данные факторы и являются характеристикой вкусов и предпочтений пользователей.




%% Вспомогательные команды - Additional commands
%
%\newpage % принудительное начало с новой страницы, использовать только в конце раздела
%\clearpage % осуществляется пакетом <<placeins>> в пределах секций
%\newpage\leavevmode\thispagestyle{empty}\newpage % 100 % начало новой страницы